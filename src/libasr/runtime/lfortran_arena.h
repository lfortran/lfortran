/*
 * Arena Allocator for LFortran Runtime
 *
 * Fast bump-pointer allocation for local fixed-size arrays.
 * Avoids stack overflow while maintaining near-stack performance.
 *
 * Design based on: github.com/certik/wasi and Ryan Fleury's arena pattern.
 *
 * Two thread-local scratch arenas enable conflict-free nested allocation:
 *   - Parent and child functions can safely pass arrays between them
 *   - Alternating arenas prevent child allocations from corrupting parent data
 *   - Arbitrarily deep call stacks work with just 2 arenas
 *
 * Thread Safety:
 *   - All arena state (scratch_arenas, scratch_current_arena, handle_pool)
 *     is thread-local (TLS), so each thread has independent arenas.
 *   - No locks or synchronization needed for typical single-threaded use.
 *   - For multi-threaded Fortran programs (OpenMP, etc.), each thread
 *     automatically gets its own arena instances via TLS.
 *   - Arena* pointers must NOT be shared between threads.
 *
 * Usage (generated by LLVM codegen):
 *   Scratch s = scratch_begin();           // or scratch_begin_avoid_conflict()
 *   void* arr = arena_alloc(s.arena, sz);  // bump-pointer allocation
 *   scratch_end(s);                        // restore arena position
 */

#ifndef LFORTRAN_ARENA_H
#define LFORTRAN_ARENA_H

#include <stddef.h>
#include <stdint.h>

#ifdef __cplusplus
extern "C" {
#endif

#ifdef _WIN32
#define LFORTRAN_ARENA_API __declspec(dllexport)
#define LFORTRAN_ARENA_TLS __declspec(thread)
#elif defined(__linux__)
#define LFORTRAN_ARENA_API __attribute__((visibility("default")))
#define LFORTRAN_ARENA_TLS __thread
#else
#define LFORTRAN_ARENA_API
#define LFORTRAN_ARENA_TLS __thread
#endif

/* Alignment for all allocations (must be power of two) */
#define ARENA_ALIGNMENT 16

/* Minimum chunk size (actual may be larger due to allocation request) */
#define ARENA_MIN_CHUNK_SIZE 4096

/* Initial size for scratch arenas (will grow as needed) */
#define ARENA_INITIAL_SIZE 4096

/* Forward declarations */
typedef struct Arena Arena;
typedef struct ArenaChunk ArenaChunk;

/*
 * Position handle for save/restore operations.
 * Captures arena state at a point in time.
 */
typedef struct {
    ArenaChunk* chunk;
    char* ptr;
} ArenaPos;

/*
 * Scratch scope for temporary allocations.
 * Created by scratch_begin(), released by scratch_end().
 */
typedef struct {
    Arena* arena;
    ArenaPos saved_pos;
} Scratch;

/* --- Arena API --- */

/*
 * Create a new arena with specified initial capacity.
 * The arena will grow automatically when more space is needed.
 * Returns NULL on allocation failure.
 */
LFORTRAN_ARENA_API Arena* arena_new(size_t initial_size);

/*
 * Allocate memory from the arena.
 * All allocations are aligned to ARENA_ALIGNMENT bytes.
 * Grows arena automatically if needed.
 * Aborts on allocation failure (never returns NULL).
 */
LFORTRAN_ARENA_API void* arena_alloc(Arena* arena, size_t size);

/*
 * Get current position in arena (for later restore).
 */
LFORTRAN_ARENA_API ArenaPos arena_get_pos(Arena* arena);

/*
 * Reset arena to a previously saved position.
 * All allocations made after that position become invalid.
 * Chunks are retained for reuse (not freed).
 */
LFORTRAN_ARENA_API void arena_reset(Arena* arena, ArenaPos pos);

/*
 * Free all memory used by the arena.
 * The arena pointer becomes invalid after this call.
 */
LFORTRAN_ARENA_API void arena_free(Arena* arena);

/* --- Scratch API (Two-Arena System) --- */

/*
 * Initialize scratch arenas. Called automatically on first use.
 * Safe to call multiple times (idempotent).
 */
LFORTRAN_ARENA_API void scratch_init(void);

/*
 * Begin a scratch scope using the first available arena.
 * Use when no other arena is in scope for persistent allocations.
 */
LFORTRAN_ARENA_API Scratch scratch_begin(void);

/*
 * Begin a scratch scope avoiding conflict with another arena.
 * Use when 'conflict' arena holds data that must survive this scope.
 * Returns a scratch using a different arena than 'conflict'.
 *
 * Example:
 *   void child(Arena* parent_arena) {
 *       Scratch s = scratch_begin_avoid_conflict(parent_arena);
 *       // allocations on s.arena won't corrupt parent_arena's data
 *       void* result = arena_alloc(parent_arena, 100);  // safe
 *       scratch_end(s);
 *       return result;
 *   }
 */
LFORTRAN_ARENA_API Scratch scratch_begin_avoid_conflict(Arena* conflict);

/*
 * End a scratch scope, restoring the arena to its pre-scratch state.
 * All allocations made during this scratch scope become invalid.
 */
LFORTRAN_ARENA_API void scratch_end(Scratch scratch);

/*
 * Get a scratch arena (for generated code that needs arena pointer).
 * Equivalent to scratch_begin().arena but without creating a scope.
 */
LFORTRAN_ARENA_API Arena* scratch_get_arena(void);

/*
 * Get a scratch arena avoiding conflict (for generated code).
 */
LFORTRAN_ARENA_API Arena* scratch_get_arena_avoid_conflict(Arena* conflict);

/* --- Conflict-Aware API for Nested Calls (Two-Arena Alternation) ---
 *
 * The pattern for nested/recursive function calls:
 *
 *   void f() {
 *       Arena* parent = scratch_get_current();      // What parent is using
 *       Arena* my_arena = scratch_get_arena_avoid_conflict(parent);
 *       scratch_set_current(my_arena);              // Mark as active
 *       ArenaPos saved = arena_get_pos(my_arena);
 *       // ... allocate locals on my_arena ...
 *       // ... call g() which will use a DIFFERENT arena ...
 *       arena_reset(my_arena, saved);               // Restore position
 *       scratch_set_current(parent);                // Restore parent view
 *   }
 *
 * This ensures f() and h() share arena 0, while g() uses arena 1.
 * Works for arbitrarily deep call stacks with only 2 arenas.
 */

/*
 * Get the currently active scratch arena (what the caller is using).
 * Returns NULL at top-level (no function has claimed an arena yet).
 */
LFORTRAN_ARENA_API Arena* scratch_get_current(void);

/*
 * Set the currently active scratch arena.
 * Called at function entry to claim an arena, and at exit to restore parent.
 */
LFORTRAN_ARENA_API void scratch_set_current(Arena* arena);

/* --- New Codegen API for Two-Arena Alternation ---
 *
 * These functions are used by LLVM codegen for the conflict-aware pattern.
 * They combine multiple operations for efficiency.
 *
 * Handle Pool Limit:
 *   - Handles are stored in a thread-local pool of 256 entries.
 *   - This limits scratch nesting depth to 256 levels per thread.
 *   - 256 is chosen as sufficient for any reasonable Fortran call depth
 *     while avoiding dynamic allocation overhead on every begin/end.
 *   - If exceeded, a fatal error is raised.
 */

/*
 * Begin a scratch scope with conflict avoidance.
 * Gets an arena different from the current active arena, saves position,
 * and sets the new arena as current.
 *
 * Returns an opaque handle encoding:
 *   - The arena being used
 *   - The saved position
 *   - The parent arena to restore
 *
 * Call _lfortran_scratch_end to complete the scope.
 *
 * Note: Maximum nesting depth is 256 (handle pool size).
 */
LFORTRAN_ARENA_API void* _lfortran_scratch_begin(void);

/*
 * Allocate from the current scratch arena.
 */
LFORTRAN_ARENA_API void* _lfortran_scratch_alloc(int64_t size);

/*
 * End a scratch scope started by _lfortran_scratch_begin.
 * Restores the arena position and sets current back to parent.
 */
LFORTRAN_ARENA_API void _lfortran_scratch_end(void* handle);

/* --- Legacy API (backward compatible with current LLVM codegen) --- */

/*
 * Thread-local arena pointer for inline access from generated code.
 */
extern LFORTRAN_ARENA_API LFORTRAN_ARENA_TLS char* _lfortran_arena_ptr;

/*
 * Initialize arena - called at program start.
 */
LFORTRAN_ARENA_API void _lfortran_arena_init(void);

/*
 * Allocate from arena with 16-byte alignment.
 */
LFORTRAN_ARENA_API void* _lfortran_arena_alloc(int64_t size);

/*
 * Save current arena position (returns opaque pointer).
 */
LFORTRAN_ARENA_API void* _lfortran_arena_save(void);

/*
 * Restore arena to previously saved position.
 */
LFORTRAN_ARENA_API void _lfortran_arena_restore(void* saved_ptr);

#ifdef __cplusplus
}
#endif

#endif /* LFORTRAN_ARENA_H */
